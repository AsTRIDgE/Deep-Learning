{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP Forest cover.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsTRIDgE/Deep-Learning/blob/master/MLP_Forest_cover.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBT2XbkMPQOP",
        "colab_type": "text"
      },
      "source": [
        "#Applying MLP to Forest Cover dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2UOfbfGPMjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the dataset from sklearn\n",
        "from sklearn.datasets import fetch_covtype\n",
        "\n",
        "# Use the MLP class defined in sklearn\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOy43lbHPYPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# When return_X_y = True, the load function return data and target instead of Bunch object.\n",
        "\n",
        "X, y = fetch_covtype(return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxmjmeNNPdmc",
        "colab_type": "code",
        "outputId": "6b08e174-196f-4077-85c7-b7d92d69e80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(type(X))\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(581012, 54)\n",
            "(581012,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVberQf3PdYE",
        "colab_type": "code",
        "outputId": "38f7d6bb-18a6-43ac-ff19-5947dfe6fbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Reduce the number of attributes, consider only first 10 attributes.\n",
        "X_10 = X[:,:10]          #no of columns is 10\n",
        "\n",
        "print(X_10.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(581012, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujc_jJP4PdNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into 90% training and 10% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# The 10% testing data obtained during this split will be take as our entire database.\n",
        "# This is because the original dataset is too big.\n",
        "\n",
        "X10_train, X10_test, y10_train, y10_test = train_test_split(X_10, y, test_size=0.1, stratify=y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCq3kVf7P0A2",
        "colab_type": "code",
        "outputId": "6c15d118-ab6e-4ae3-dd93-0f3e8fc0af80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X10_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(58102, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwXBOgjZPz8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Handle only the modified 1% dataset. Split that into training and testing.\n",
        "# X and y are updated with the downsized dataset\n",
        "X = X10_test\n",
        "y = y10_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "995j5IVFPz31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# While splitting, make an unbiased splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b7UrSBKPzxl",
        "colab_type": "code",
        "outputId": "83088220-eb24-44f0-9f40-6ee3d7af9a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Feature scaling using Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gOTaCBIQBfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale both training and testing data\n",
        "\n",
        "X_train = sc.transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EhWQWO-QBYh",
        "colab_type": "code",
        "outputId": "670817f5-ebf7-4919-fc2b-da953e5c5d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# Create an object (model) for MLP.\n",
        "mlpClassifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(30, 20), random_state=42)\n",
        "\n",
        "# Train the MLP using 80% training set\n",
        "mlpClassifier.fit(X_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(30, 20), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=42, shuffle=True, solver='lbfgs',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIidaiLZQKur",
        "colab_type": "code",
        "outputId": "a5b2fa53-14d2-4122-9f8a-73fcf970054b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The classification score accuracy \n",
        "score = mlpClassifier.score(X_test, y_test)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7471818260046468\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}